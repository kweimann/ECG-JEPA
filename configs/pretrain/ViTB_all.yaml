# learnable encoder parameters: 85,185,024
# data
sampling_frequency: 500
channels: [I, II, III, AVR, AVL, AVF, V1, V2, V3, V4, V5, V6]
channel_size: 5000
patch_size: 25
min_block_size: 10
min_keep_ratio: 0.15
max_keep_ratio: 0.25
datasets:
  mimic-iv-ecg: 0.7
  code-15: 0.1
  ptb-xl: 0.05
  ptb: 0.01875
  cpsc: 0.025
  cpsc-extra: 0.0125
  georgia: 0.0375
  ningbo: 0.028125
  chapman-shaoxing: 0.009375
  st-petersburg: 0.01875
# model architecture
dim: 768
depth: 12
num_heads: 12
pred_dim: 384
pred_depth: 12
pred_num_heads: 12
mlp_ratio: 4.
qkv_bias: False
dropout: 0.
attn_dropout: 0.
num_registers: 1
bias: False
norm_eps: 1.0e-6
layer_scale_eps: 0.
# training
steps: 100_000
batch_size: 512
encoder_momentum: 0.998
final_encoder_momentum: 0.9995
learning_rate: 1.0e-3
final_learning_rate: 1.0e-6
learning_rate_warmup_steps: 10_000
weight_decay: 1.0e-2
final_weight_decay: 1.0e-1
opt_betas: [0.9, 0.99]
opt_eps: 1.0e-6
gradient_clip: 0
gradient_accumulation_steps: 2
checkpoint_interval: 10_000